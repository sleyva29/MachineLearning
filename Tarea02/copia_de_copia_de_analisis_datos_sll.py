# -*- coding: utf-8 -*-
"""Copia de Copia de  Analisis_datos_SLL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTcOyUnYHBrap_cL33kYsaw_M3gayBes

## Importar librerías
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from math import floor
import pandas as pd
import sys
from tqdm import tqdm
import seaborn as sns
import random
from statistics import mode

"""# Análisis de datos

Función para leer dataframe
"""

def cargar_df():
    path_ = input('Ingresar ruta de acceso del archivo') 
    sep_ = input('Ingresar caracter de separación:')
    df_load = pd.read_csv(path_, sep=sep_)
    return df_load

"""Función para calcular el número de atributos y los atributos contenidos en el dataframe"""

def Numero_atributos(df_na):
    num_atributos_ = df_na.shape[1]    #número de columnas
    atributos_ = list(df_na.columns)   #extrae columnas-->lista

    print(f'El dataset tiene {num_atributos_} atributos:')
    
    for i in range(num_atributos_):   
        print(f'{i+1}.- {atributos_[i]} ')
    return num_atributos_, atributos_

"""Función para obtener los atributos, sus observaciones y el tipo de atributo"""

def Observaciones(df_o, atributos_o):
    observaciones_ = {}
    for i, atributo in enumerate(atributos_o):
        obs_ = df_o[atributo].value_counts() #Return a Series containing counts of unique values.
        obs_l_ = list(obs_.index)            #Lista de los valores únicos encontrados
        obs_ = pd.DataFrame(obs_.index)      

        tipo = df_o[atributo].dtypes

        if tipo == 'object': tipo = 'categórico'
        elif tipo == 'int': tipo = 'entero'
        elif tipo == 'float64': tipo = 'flotante'

        print(f'''El atributo {atributo} es de tipo {tipo}, contiene {len(obs_l_)} observaciones y son las siguientes:
        {obs_l_}
              ''')
        observaciones_.update({atributo:obs_})    
    return observaciones_

"""Función para calcular el número de instancias"""

def Numero_instancias(df_ni):
    num_instancias_ = df_ni.shape[0]
    print(f'El dataset tiene {num_instancias_} instancias')
    return num_instancias_

"""Función para conocer el número de datos faltantes"""

def Datos_Faltantes(df_):
    faltantes_df = df_.isnull().sum()
    print(f'Datos faltantes por atributo:\n {faltantes_df}')
    #return faltantes_df

"""# Relación entre atributos"""

def cuartiles(df_, atributo_):
#1. 25% de los datos es menor que o igual a este valor.
#2. La mediana. 50% de los datos es menor que o igual a este valor.
#3. 75% de los datos es menor que o igual a este valor.
#rango intercuartil. La distancia entre el primer 1er cuartil y el 3er 
#cuartil (Q3-Q1); de esta manera, abarca el 50% central de los datos.

#ordenar los elementos de la columna
  v = df_[atributo_]
  v = np.asarray(v.sort_values())
  n = len(v)
  if n % 2 == 0:
 
    #calcular cuartil uno 
    a = int(n/4) 
    q1 = v[a]
    #calcular cuartil dos
    b = (n)/2
    c = b - 1
    q2 = (v[b] + v[c])/2
    #calcular cuartil tres
    d = int((3*n)/4)
    q3 = v[d]
  else:

    #calcular cuartil dos
    b = int(n/2)
    q2 = v[b]
    
    if b % 2 == 0:
      #calcular cuartil uno 
      a = int(n/4) 
      q1 = (v[a] + v[a-1])/2
      #calcular cuartil tres 
      d = int((3*(n))/4) 
      q3 = (v[a] + v[d-1])/2
    else:

      #calcular cuartil uno 
      a = int(n/4) 
      q1 = v[a]
      #calcular cuartil tres
      d = int((3*n)/4)
      q3 = v[d]

  print(f'Q1: {q1}, Q2: {q2}, Q3: {q3}')
  return q1, q2, q3

def outlier(df_,atributo_):
  v = df_[atributo_]
  q1, q2, q3 = cuartiles(df_,atributo_)
  out = []
  for valor in v:
    if valor < q1:
      if abs(valor-q1) > 1.5*(q3-q1):
        out.append(valor)
    if valor > q3:
      if abs(valor-q3) > 1.5*(q3-q1):
        out.append(valor)
    
  if len(out) == 0:
    print(f'No existen valores atípicos en el atributo {atributo_}')
  else:
    print(f'Se encontraron {len(out)} valores atípicos')
    print(f'Los valores atípicos encontrados son: {np.asarray(out)}')
  return out

def box_plot(df_, atributo_):
  fig, ax = plt.subplots(figsize = (15,13))
  plt.boxplot(df[atributo_])
  plt.title(f'Distribución de {atributo_}', fontsize = 18)
  plt.show()

def box_plot_sns(df_, atributo_):
  fig, ax = plt.subplots(figsize = (13,8))
  ax = sns.boxplot(x=df['BMI'], data=df)
  plt.title(f'Distribución de {atributo_}', fontsize = 18)
  plt.show()

def Distribucion(df_, atributo_, atributo_d):
  fig, ax = plt.subplots(figsize = (13,10))
  sns.histplot(data = df_, x = atributo_, hue = atributo_d, ax = ax, kde=True, element = 'step')
  plt.title(f'Distribución de {atributo_}', fontsize = 18)
  ax.set_xlabel(atributo_)
  ax.set_ylabel("Frequencia")
  plt.show()

def Balance_clases(df):

  atributo_d = input('¿Cuál es tu atributo de decisión?:')
  clases = df[atributo_d]
  cl = clases.value_counts() 
  cl_ = list(cl.index)

  porcentajes_ = (cl*100)/sum(cl)
  print(porcentajes_)

  print(f'El dataset cuenta con {len(cl_)} clases y son las siguientes: {cl_}')

"""#CARGAR DATASET ORIGINAL"""

from google.colab import drive
drive.mount('/content/drive')

sys.path.append('/content/drive/MyDrive/Machine_Learning')

#/content/drive/MyDrive/Machine_Learning/Tarea01/heart_2020_cleaned (1).csv
#/content/drive/MyDrive/Machine_Learning/Tarea01/codificado_noindex (1).csv
#/content/drive/MyDrive/Machine_Learning/HeartDisease.csv
df = cargar_df()

df.head()

"""#Adquisición de información"""

num_atributos, atributos = Numero_atributos(df)

num_instancias = Numero_instancias(df)

observaciones = Observaciones(df, atributos)

Datos_Faltantes(df)

cuartiles(df,'BMI')

Balance_clases(df)

#outlier(df,'BMI')
#outlier(df,'PhysicalHealth')

Distribucion(df,'BMI','HeartDisease')

"""# Normalización"""

def norm_medias(df_m):
    df_min_m = df_m.min() 
    df_max_m = df_m.max()
    df_mean_m = df_m.mean()
    df_nm = (df_m - df_mean_m) / (df_max_m - df_min_m)
    return df_nm

#lim_sup y lim_inf son los nuevos límites
def norm_min_max(df_atributo, lim_inf, lim_sup):
    df_atri_min = df_atributo.min()
    df_atri_max = df_atributo.max()
    rango = lim_sup - lim_inf
    df_norm_min_max = (((df_atributo - df_atri_min)/(df_atri_max - df_atri_min)) * rango) + lim_inf
    return df_norm_min_max

def norm_z_score(df_z):
    df_z_mean = df_z.mean()
    df_z_std = df_z.std() 
    df_nz = (df_z - df_z_mean) / df_z_std
    return df_nz

def Distribucion_atributo(df_, atributo_,name_atributo, atributo_d):
  type(name_atributo)
  fig, ax = plt.subplots(figsize = (13,10))
  sns.histplot(data = df_, x = atributo_, hue = atributo_d, ax = ax, kde=True, element = 'step')
  plt.title(f'Distribución de {name_atributo}', fontsize = 18)
  ax.set_xlabel(name_atributo)
  ax.set_ylabel("Frequencia")
  plt.show()

"""Distribución de atributo BMI normalización z-score"""

atributo_norm_z_score = norm_z_score(df['BMI'])

atri_decision = df['HeartDisease']

Distribucion_atributo(df,atributo_norm_z_score,'BMI',atri_decision)

"""Distribución de atributo BMI normalización minmax"""

atributo_norm_minmax = norm_min_max(df['BMI'],0,1)

Distribucion_atributo(df,atributo_norm_minmax,'BMI',atri_decision)

"""Distribución de atributo BMI normalización por medias"""

atributo_norm_medias = norm_medias(df['BMI'])

Distribucion_atributo(df,atributo_norm_medias,'BMI',atri_decision)

"""# Generar datos faltantes"""

def Faltantes(dataf):
    df_ = dataf.copy()
    faltantes_df = df_.isnull().sum()
    print(f'Datos faltantes por atributo:\n{faltantes_df}')

    if faltantes_df.sum() != 0:
      #calcular el porcentaje de datos faltantes
      num_instancias = df_.shape[0]
      percent = (faltantes_df/num_instancias)*100
      print('Porcentaje de datos faltantes por atributo %\n', percent)
    
      co = df_.columns.tolist()
      lista = []
      for column in co:
        if percent[column] >= 60:
          lista.append(column)
          print(f'El porcentaje de datos faltantes de los atributos: {lista} es mayor al 60%, se recomienda eliminar el atributo')

    else:
      print('No hay datos faltantes')
      yn = input('¿Desea asignar valores faltantes? (si/no) ')
      if yn == 'si':
        atributo_ = input('¿Qué atributo desea trabajar? ')
        opcion = float(input('¿Qué porcentaje de datos desea de datos faltantes? Agregar solo número del 0 al 100: '))
        long = df_[atributo_].shape[0]
        indices = random.sample(range(0, long - 1), int((opcion*long)/100))
        df_[atributo_][indices] = np.nan
    return df_

Datos_Faltantes(df)

"""No hay datos faltantes en el dataframe original

# CARGAR DF CODIFICADO
"""

#/content/drive/MyDrive/Machine_Learning/Tarea01/heart_2020_cleaned (1).csv
#/content/drive/MyDrive/Machine_Learning/Tarea01/codificado_noindex (1).csv
#/content/drive/MyDrive/Machine_Learning/HeartDisease.csv
df_codificado = cargar_df()

df_codificado.head()

"""#Imputación

IMPUTACIÓN POR MEDIAS
"""

df_falta_ = Faltantes(df_codificado)

"""Resultado de haber utilizado función Faltantes"""

Datos_Faltantes(df_falta_)

def imputacion_media(df_,atributo_):
  dfi = df_.copy()
  check_for_nan = dfi[atributo_].isnull().values.any()
  if check_for_nan == False:
    print('No hay datos faltantes')
  media_atr = dfi[atributo_].mean()
  dfi[atributo_] = dfi[atributo_].fillna(media_atr)

  return dfi

df_imedia = imputacion_media(df_falta_,'BMI')

df_imedia

"""IMPUTACIÓN POR MODA"""

def imputacion_moda(df_,atributo_):
  dfi = df_.copy()
  check_for_nan = dfi[atributo_].isnull().values.any()
  if check_for_nan == False:
    print('No hay datos faltantes')
  moda_atr = mode(dfi[atributo_])
  dfi[atributo_] = dfi[atributo_].fillna(moda_atr)

  return dfi

"""IMPUTACIÓN POR MEDIA DE CLASES"""

def imputacion_media_clases(df_, atributo_imputacion, atributo_d):
  df_mc = df_.copy()
  check_for_nan = df_mc[atributo_imputacion].isnull().values.any()
  if check_for_nan == False:
    print('No hay datos faltantes')

  c = df_mc[atributo_d]
  cl = c.value_counts()
  clases = list(cl.index)
  
  for i in range(len(clases)):
    inst_ = df_mc[df_mc[atributo_d] == clases[i]]
    print(clases[i])
    media_clase = inst_[atributo_imputacion].mean()
    df_mc[atributo_imputacion] = df_mc[atributo_imputacion].fillna(media_clase)
    print(media_clase)

  return df_mc

df_mediac = imputacion_media_clases(df_falta_,'BMI','HeartDisease')
df_mediac

Datos_Faltantes(df_mediac)

df_mediac['BMI'].mean()

fig, ax = plt.subplots(figsize = (13,10))
sns.histplot(data = df_mediac, x = 'BMI', ax = ax, kde=True, element = 'step')
plt.title(f'Distribución de imputacion por media de clases', fontsize = 18)
ax.set_xlabel('BMI')
ax.set_ylabel("Frequencia")
plt.show()

"""IMPUTACION POR REGRESIÓN"""

plt.figure(figsize=(20,5))

plt.subplot(1,3,1)
plt.scatter(df['BMI'],df['HeartDisease'],color='orange',marker="v")
plt.xlabel('BMI')
plt.ylabel('HeartDisease')

def MSE(h, Y):
  return np.sum((h - Y) ** 2) / (2 * (len(h)))

def aplica_hipotesis(X, theta_0, theta_1):
  return theta_1 * X + theta_0

def costo(X, Y, theta_0, theta_1):
  h = aplica_hipotesis(X, theta_0, theta_1)
  cost = MSE(h, Y)
  return h, cost
  
def plot(X, Y, theta_0, theta_1):
  x = np.linspace(0, X.max(), len(X))
  plt.plot(X, Y, marker='.', linestyle='none', label='Y data')
  plt.plot(x, aplica_hipotesis(x, theta_0, theta_1), label='model line')
  # plt.plot(X, aplica_modelo_lineal(x, theta_0, theta_1), marker='.', linestyle='none', label='h')
  plt.legend(loc="upper left")
  plt.show()

def graf_rl(xi, yi, theta0, theta1):
  x = np.linspace(0, xi.max(),len(xi))
  plt.plot(xi, yi, marker='.', linestyle='none', label= 'Y data')
  plt.plot(x, aplica_hipotesis(x, theta0, theta1), label = 'Hipótesis')
  plt.suptitle('Regresión lineal')
  #plt.xlabel('Medio de comunicación')
  plt.legend(loc="upper left")
  plt.show()

X = np.asarray(df['BMI'])

theta0, theta1 = 0.01, 0.01
X = np.asarray(df_codificado['BMI'])
Y = np.asarray(df_codificado['HeartDisease'])

h, c = costo(X, Y, theta0, theta1)
graf_rl(X, Y, theta0, theta1)

print('MSE = ', c)

"""IMPUTACIÓN ALEATORIA"""

df_a = df_falta_.copy()
faltantes_df = df_a['BMI'].isnull().sum()
no_faltantes = df_a[df_a['BMI'].notnull()]

min = no_faltantes['BMI'].min()
max = no_faltantes['BMI'].max()
random_float_list = []
for i in range(faltantes_df):
  x = round(random.uniform(min,max),2)
  random_float_list.append(x)
df_a.loc[df_a['BMI'].isnull(),df_a['BMI']] = random_float_list

Distribucion_atributo(df_aleatorio_bmi,df_aleatorio_bmi['BMI'],'BMI',df_aleatorio_bmi['HeartDisease'])

Distribucion(df,'BMI','HeartDisease')

"""#I. ALAEATORIA PHYSICALACTIVITY"""

df_aleatorio_PH = imputacion_aleatoria(df_falta_, 'PhysicalActivity')

Datos_Faltantes(df_aleatorio_PH)

Distribucion_atributo(df_aleatorio_PH,df_aleatorio_PH['PhysicalActivity'],'PhysicalActivity',df_aleatorio_PH['HeartDisease'])

PhysicalHealth

